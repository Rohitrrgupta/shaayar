{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "110d24b7",
   "metadata": {},
   "source": [
    "Creating an LSTM (Long Short-Term Memory) model for poem generation involves several steps. LSTM is a type of recurrent neural network that is particularly well-suited for sequence-to-sequence tasks like generating poems. Here's a step-by-step guide to building an LSTM model for poem creation:\n",
    "\n",
    "1. **Data Collection**:\n",
    "   Gather a large dataset of poems that will serve as the training data for your model. You can find poems from online sources, public domain poetry collections, or even create your own dataset.\n",
    "\n",
    "2. **Data Preprocessing**:\n",
    "   Prepare the raw text data for training. This involves tasks such as tokenization, lowercasing, removing punctuation, and creating numerical representations of the words. You can use libraries like NLTK or TensorFlow Tokenizer for this step.\n",
    "\n",
    "3. **Create Sequences**:\n",
    "   Convert the processed text into sequences of fixed length. For instance, if your LSTM model takes sequences of 50 words as input, divide the entire poem dataset into overlapping sequences of 50 words.\n",
    "\n",
    "4. **Split Data into Train and Test Sets**:\n",
    "   Divide the dataset into training and testing sets. The training set will be used to train the LSTM model, while the testing set will help evaluate its performance.\n",
    "\n",
    "5. **Build LSTM Model**:\n",
    "   Set up the architecture of your LSTM model. In Keras or TensorFlow, you can use the `LSTM` layer along with `Embedding` and `Dense` layers to create the model.\n",
    "\n",
    "6. **Compile Model**:\n",
    "   Specify the loss function and optimization algorithm for the LSTM model using the `compile` method. Since poem generation is a language generation task, you can use categorical cross-entropy as the loss function.\n",
    "\n",
    "7. **Train Model**:\n",
    "   Train the LSTM model on the prepared training dataset. Adjust the number of epochs and batch size according to your dataset size and computing resources. You might need to experiment with hyperparameters to achieve good results.\n",
    "\n",
    "8. **Generate Poems**:\n",
    "   After training the model, you can use it to generate poems. Start with a seed sequence, and use the trained model to predict the next word in the sequence. Then, use the predicted word to update the sequence, and repeat the process to generate the desired length of the poem.\n",
    "\n",
    "9. **Evaluate and Iterate**:\n",
    "    Evaluate the quality of the generated poems based on various metrics like coherence, grammar, and overall poetic quality. Iterate on your model, data preprocessing, or hyperparameters to improve the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e872df79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import os\n",
    "import random\n",
    "import nltk\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b54c7cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving list of names of txt files\n",
    "directory_path = r'C:\\Users\\rohit\\Python\\Personal Projects\\shaayar\\ghazal_data\\all'\n",
    "\n",
    "txt_files = [file for file in os.listdir(directory_path) if file.endswith('.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5cdc467",
   "metadata": {},
   "outputs": [],
   "source": [
    "poems_data = \"\"\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    file_path = os.path.join(directory_path, txt_file)\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        poem_content = file.read()\n",
    "    \n",
    "    poems_data += poem_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "455e37d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gulon ko sunna zara tum sadaen bheji hain\\ngulon ke haath bahut si duaen bheji hain\\n\\njo aftab kabhi bhi ghurub hota nahin\\nhamara dil hai usi ki shuaen bheji hain\\n\\nagar jalae tumhen bhi shifa mile shayad\\nik aise dard ki tum ko shuaen bheji hain\\n\\ntumhari khushk si ankhen bhali nahin lagtin\\nvo saari chizen jo tum ko rulaen, bheji hain\\n\\nsiyah rang chamakti hui kanari hai\\npahan lo achchhi lagengi ghaTaen bheji hain\\n\\ntumhare khvab se har shab lipaT ke sote hain\\nsazaen bhej do ham ne khataen bheji hain\\n\\nakela patta hava men bahut buland uá¸Œa\\nzamin se paanv uThao havaen bheji hain\\nkoi hua na ru-kash Tak meri chashm-e-tar se\\nkya kya na abr aa kar yaan zor zor barse\\n\\nvahshat se meri yaaro khatir na jama rakhiyo\\nphir aave ya na aave nau pur uTha jo ghar se\\n\\nab juun sarishk un se phirne ki chashm mat rakh\\njo khaak men mile hain gir kar tiri nazar se\\n\\ndidar khvah us ke kam hon to shor kam ho\\nhar subh ik qayamat uThti hai us ke dar se\\n\\ndaagh ek ho jila bhi khuun ek ho baha bhi\\nab bahs kya hai dil se k'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poems_data[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7d5d7cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2005787"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(poems_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b0e3207",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = poems_data.lower().split('\\n')\n",
    "corpus = [line.replace('-', ' - ') + ' \\n' for line in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d220716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gulon ko sunna zara tum sadaen bheji hain \\n',\n",
       " 'gulon ke haath bahut si duaen bheji hain \\n',\n",
       " ' \\n',\n",
       " 'jo aftab kabhi bhi ghurub hota nahin \\n',\n",
       " 'hamara dil hai usi ki shuaen bheji hain \\n']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84f660fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19023"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words = 100)\n",
    "#tokenizer = Tokenizer() #for better accuracy\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c114540f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seqs = []\n",
    "for line in corpus:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram = token_list[:i+1]\n",
    "        input_seqs.append(n_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6158181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_len = max([len(x) for x in input_seqs])\n",
    "max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfd23601",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seqs = np.array(pad_sequences(input_seqs, maxlen = max_seq_len, padding = 'pre'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aedfc69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = input_seqs[:, :-1]\n",
    "y_train = input_seqs[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99ca21b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes = total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd6b3ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160167, 16)\n",
      "(160167, 19023)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0af51d64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5006/5006 [==============================] - 1029s 205ms/step - loss: 3.9360 - accuracy: 0.1074\n",
      "Epoch 2/20\n",
      "5006/5006 [==============================] - 678s 135ms/step - loss: 3.7404 - accuracy: 0.1320\n",
      "Epoch 3/20\n",
      "5006/5006 [==============================] - 667s 133ms/step - loss: 3.6880 - accuracy: 0.1374\n",
      "Epoch 4/20\n",
      "5006/5006 [==============================] - 636s 127ms/step - loss: 3.6558 - accuracy: 0.1410\n",
      "Epoch 5/20\n",
      "5006/5006 [==============================] - 616s 123ms/step - loss: 3.6326 - accuracy: 0.1451\n",
      "Epoch 6/20\n",
      "5006/5006 [==============================] - 618s 123ms/step - loss: 3.6142 - accuracy: 0.1476\n",
      "Epoch 7/20\n",
      "5006/5006 [==============================] - 617s 123ms/step - loss: 3.5977 - accuracy: 0.1501\n",
      "Epoch 8/20\n",
      "5006/5006 [==============================] - 620s 124ms/step - loss: 3.5843 - accuracy: 0.1520\n",
      "Epoch 9/20\n",
      "5006/5006 [==============================] - 632s 126ms/step - loss: 3.5719 - accuracy: 0.1537\n",
      "Epoch 10/20\n",
      "5006/5006 [==============================] - 634s 127ms/step - loss: 3.5598 - accuracy: 0.1562\n",
      "Epoch 11/20\n",
      "5006/5006 [==============================] - 632s 126ms/step - loss: 3.5496 - accuracy: 0.1576\n",
      "Epoch 12/20\n",
      "5006/5006 [==============================] - 630s 126ms/step - loss: 3.5392 - accuracy: 0.1591\n",
      "Epoch 13/20\n",
      "5006/5006 [==============================] - 629s 126ms/step - loss: 3.5293 - accuracy: 0.1606\n",
      "Epoch 14/20\n",
      "5006/5006 [==============================] - 635s 127ms/step - loss: 3.5191 - accuracy: 0.1633\n",
      "Epoch 15/20\n",
      "5006/5006 [==============================] - 631s 126ms/step - loss: 3.5089 - accuracy: 0.1642\n",
      "Epoch 16/20\n",
      "5006/5006 [==============================] - 633s 126ms/step - loss: 3.4990 - accuracy: 0.1664\n",
      "Epoch 17/20\n",
      "5006/5006 [==============================] - 634s 127ms/step - loss: 3.4898 - accuracy: 0.1679\n",
      "Epoch 18/20\n",
      "5006/5006 [==============================] - 628s 125ms/step - loss: 3.4798 - accuracy: 0.1701\n",
      "Epoch 19/20\n",
      "5006/5006 [==============================] - 620s 124ms/step - loss: 3.4693 - accuracy: 0.1723\n",
      "Epoch 20/20\n",
      "5006/5006 [==============================] - 623s 125ms/step - loss: 3.4588 - accuracy: 0.1744\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 120, input_length = max_seq_len - 1))\n",
    "model.add(Bidirectional(LSTM(100)))\n",
    "model.add(Dense(total_words, activation = 'softmax'))\n",
    "adam = Adam(lr = 0.1)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = adam, metrics = ['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs = 20, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d4ea24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A more complex and better model for better text generation\n",
    "\n",
    "#model = Sequential()\n",
    "#model.add(Embedding(total_words, 240, input_length = max_seq_len - 1))\n",
    "#model.add(Bidirectional(LSTM(150)))\n",
    "#model.add(Bidirectional(LSTM(100, return_sequences = True)))\n",
    "#model.add(Dense(total_words, activation = 'softmax'))\n",
    "#adam = Adam(lr = 0.001)\n",
    "#model.compile(loss = 'categorical_crossentropy', optimizer = adam, metrics = ['accuracy'])\n",
    "#history = model.fit(X_train, y_train, epochs = 200, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38589b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uski aakhein jaise to hai e e ka hai e ka hai e ka hai e ka hai koi ho aur ho ho\n"
     ]
    }
   ],
   "source": [
    "seed_text = 'Uski aakhein jaise'\n",
    "next_word = 20\n",
    "\n",
    "for _ in range(next_word):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen = max_seq_len - 1, padding = 'pre')\n",
    "    predicted_probs = model.predict(token_list, verbose = 0)\n",
    "    predicted_index = np.argmax(predicted_probs)\n",
    "    \n",
    "    output_word = ''\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted_index:\n",
    "            output_word = word\n",
    "            break\n",
    "    seed_text += ' ' + output_word\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4519797",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
